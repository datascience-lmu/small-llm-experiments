{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93cfcd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4fb9838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c40d526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 4397\n",
      "[[0.30769231 0.69230769]\n",
      " [0.34188034 0.65811966]\n",
      " [0.34615385 0.65384615]\n",
      " [0.52739726 0.47260274]]\n",
      "[[0.32013201 0.67986799]\n",
      " [0.30506478 0.69493522]\n",
      " [0.2426384  0.7573616 ]\n",
      " [0.38943894 0.61056106]]\n",
      "[[-0.01243971  0.01243971]\n",
      " [ 0.03681556 -0.03681556]\n",
      " [ 0.10351545 -0.10351545]\n",
      " [ 0.13795832 -0.13795832]]\n"
     ]
    }
   ],
   "source": [
    "with open('gpt_5_string_2.txt', 'r') as f:\n",
    "    string = f.read()\n",
    "    print(f\"len: {len(string)}\")\n",
    "    (a, b) = experiments.test_markov_prop(string, 2, 0.2)\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86369d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66c2df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "def add_characters(input: str, char: str = \"x\") -> str:\n",
    "    return \"\".join(\n",
    "        list(\n",
    "            itertools.chain.from_iterable(\n",
    "                zip(input, itertools.repeat(char, times=len(input)))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# add random length\n",
    "def add_words(input: str, word_length: int = 6) -> str:\n",
    "    input_split = input.split(\" \")\n",
    "    junk = []\n",
    "    for _ in range(len(input_split)):\n",
    "        word = \"\"\n",
    "        for _ in range(word_length):\n",
    "            word += random.choice(string.ascii_letters)\n",
    "        junk.append(word)\n",
    "    return \" \".join(list(itertools.chain.from_iterable(zip(input_split, junk))))\n",
    "\n",
    "\n",
    "def split_across_messages(input: str) -> list[str]:\n",
    "    input_split = input.split(\" \")\n",
    "\n",
    "    return input_split\n",
    "\n",
    "\n",
    "def scramble_word(input: str) -> str:\n",
    "    words = input.split()\n",
    "    res = []\n",
    "\n",
    "    for word in words:\n",
    "        if len(word) > 2:\n",
    "            inner_letters = list(word)[1:-1]\n",
    "\n",
    "            random.shuffle(inner_letters)\n",
    "\n",
    "            new_word = word[0]\n",
    "            new_word += \"\".join(inner_letters)\n",
    "            new_word += word[-1]\n",
    "\n",
    "            res.append(new_word)\n",
    "\n",
    "        else:\n",
    "            res.append(word)\n",
    "\n",
    "    return \" \".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dbbf9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lxoxrxexmx xixpxsxuxmx xdxoxlxoxrx xsxixtx'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_characters(\"Do you understand me?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9d8d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lorem ocCCWh ipsum LXkRHb dolor CynVEE sit HAjqRe'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_words(\"Do you understand me?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958bd371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem', 'ipsum', 'dolor', 'sit']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_across_messages(\"Do you understand me?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f3eea72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do you udnensatrd me?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scramble_word(\"Do you understand me?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5049b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmolChatbot:\n",
    "    def __init__(self, model_name: str = \"HuggingFaceTB/SmolLM3-3B\", system_prompt: str = \"\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.history: list[dict[str, str]] = []\n",
    "\n",
    "        if system_prompt:\n",
    "            self.history.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    def reset(self, system_prompt: str = \"\"):\n",
    "        self.history = []\n",
    "\n",
    "        if system_prompt:\n",
    "            self.history.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    def __call__(self, user_input: str, enable_thinking=True):\n",
    "        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n",
    "\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=enable_thinking,\n",
    "        )\n",
    "\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][\n",
    "            len(inputs.input_ids[0]) :\n",
    "        ].tolist()\n",
    "\n",
    "        try:\n",
    "            # rindex finding 128003 (</think>)\n",
    "            split_index = len(response_ids) - response_ids[::-1].index(128003)\n",
    "        except ValueError:\n",
    "            split_index = 0\n",
    "\n",
    "        _reasoning_content = self.tokenizer.decode(\n",
    "            response_ids[:split_index], skip_special_tokens=True\n",
    "        ).strip(\"\\n\")\n",
    "        content = self.tokenizer.decode(\n",
    "            response_ids[split_index:], skip_special_tokens=True\n",
    "        ).strip(\"\\n\")\n",
    "\n",
    "        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d16a7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smol = SmolChatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bd011ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(smol(\"Hello\", enable_thinking=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73ca8c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smol.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "588b93eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QwenChatbot:\n",
    "    def __init__(self, model_name: str = \"Qwen/Qwen3-0.6B\", system_prompt: str = \"\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.history: list[dict[str, str]] = []\n",
    "\n",
    "        if system_prompt:\n",
    "            self.history.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    def reset(self, system_prompt: str = \"\"):\n",
    "        self.history = []\n",
    "\n",
    "        if system_prompt:\n",
    "            self.history.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    def __call__(self, user_input: str, enable_thinking=True):\n",
    "        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n",
    "\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=enable_thinking,\n",
    "        )\n",
    "\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][\n",
    "            len(inputs.input_ids[0]) :\n",
    "        ].tolist()\n",
    "\n",
    "        try:\n",
    "            # rindex finding 151668 (</think>)\n",
    "            split_index = len(response_ids) - response_ids[::-1].index(151668)\n",
    "        except ValueError:\n",
    "            split_index = 0\n",
    "\n",
    "        _reasoning_content = self.tokenizer.decode(\n",
    "            response_ids[:split_index], skip_special_tokens=True\n",
    "        ).strip(\"\\n\")\n",
    "        content = self.tokenizer.decode(\n",
    "            response_ids[split_index:], skip_special_tokens=True\n",
    "        ).strip(\"\\n\")\n",
    "\n",
    "        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "        return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59ddab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen = QwenChatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "888c9e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! ðŸ˜Š How can I assist you today?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen.reset()\n",
    "qwen(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "34636861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice has twenty apples, and Bob has fourteen apples. To find the total number of apples they have together, we need to add the two quantities.\n",
      "\n",
      "$$\n",
      "20 + 14 = 34\n",
      "$$\n",
      "\n",
      "Thus, the total number of apples they have is:\n",
      "\n",
      "$$\n",
      "\\boxed{34}\n",
      "$$\n",
      "[{'role': 'user', 'content': 'Alice has twenty apples and Bob has fourteen apples. How many apples do they have in total?'}, {'role': 'assistant', 'content': \"<think>\\nOkay, let's see. Alice has twenty apples, and Bob has fourteen. The question is asking how many apples they have in total. Hmm, so I need to add these two numbers together. Let me think. \\n\\nFirst, I should check if there's any trick here. Like, maybe they have some apples in common? Wait, no, the problem just says Alice has twenty and Bob has fourteen. It doesn't mention anything about overlapping apples. So maybe they are separate? Like, they don't share any apples? That seems logical. So if Alice has 20 and Bob has 14, adding them together would be 20 + 14. \\n\\nLet me do the math. 20 plus 14... Well, 20 plus 10 is 30, and then plus 4 more is 34. So that would be 34 apples in total. \\n\\nWait, but maybe I should double-check. Let me add 20 and 14 again. 20 + 10 is 30, then 30 + 4 is 34. Yeah, that seems right. \\n\\nAlternatively, I can think of it as 14 + 20. Since 14 is 10 + 4, adding 20 would be 10 + 4 + 20, which is still 34. \\n\\nI don't think there's any other way to interpret the problem. Unless there's some hidden detail I'm missing, like maybe they have some apples that are shared or something. But the problem doesn't mention anything about that. It just states their individual quantities. \\n\\nSo I think the answer is 34. Let me just confirm once more. 20 apples to Alice, 14 to Bob. 20 + 14 = 34. Yep, that's correct. \\n\\n**Final Answer**\\nThey have a total of \\\\boxed{34} apples.\\n</think>\\n\\nAlice has twenty apples, and Bob has fourteen apples. To find the total number of apples they have together, we need to add the two quantities.\\n\\n$$\\n20 + 14 = 34\\n$$\\n\\nThus, the total number of apples they have is:\\n\\n$$\\n\\\\boxed{34}\\n$$\"}]\n"
     ]
    }
   ],
   "source": [
    "qwen.reset()\n",
    "print(\n",
    "    qwen(\n",
    "        \"Alice has twenty apples and Bob has fourteen apples. How many apples do they have in total?\"\n",
    "    )\n",
    ")\n",
    "print(qwen.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b2b96878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aicle has tewtny appels and Bob has furteeon alpesp. How mnay alppes do tehy hvae in taolt?'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scramble_word(\n",
    "    \"Alice has twenty apples and Bob has fourteen apples. How many apples do they have in total?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d611831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acile has \"tenth aeppls\" and Bob has \"futeeorn aslepp\". If \"aeppls\" is interpreted as \"apples\" and \"futeeorn\" as \"futeeapple\", then each has **one apple**. Therefore, the total number of alpeps is:\n",
      "\n",
      "$$\n",
      "\\boxed{2}\n",
      "$$\n",
      "[{'role': 'user', 'content': 'Acile has ttenwy aeppls and Bob has futeeorn aslepp. How many alpeps do they hvae in talto?'}, {'role': 'assistant', 'content': '<think>\\nOkay, let\\'s see. The question is about figuring out how many \"alpeps\" are there in total between Acile and Bob. The original statements given are:\\n\\n- Acile has ttenwy aeppls\\n- Bob has futeeorn aslepp\\n\\nHmm, first, I need to make sense of these words. The key here is to translate them into English and then count the number of \"alpeps\" they each have. Let me start with Acile\\'s case.\\n\\nStarting with \"ttenwy aeppls\". The word \"ttenwy\" â€“ that seems like a typo or a misspelling. Maybe \"tenth\" or \"ten\" but with some extra letters? Let me think. \"Ttenwy\" could be \"tenth\" with an extra \"y\"? Or maybe it\\'s \"ten\" with an \"w\"? Alternatively, perhaps it\\'s \"tenth\" but written with a different spelling. Let me check if \"tenth\" is the correct term here.\\n\\nIf \"tenth\" is the correct term, then \"ttenwy\" might be a typo. Let me assume that \"tenth\" is the intended meaning. So Acile has \"tenth aeppls\". Now, \"aeppls\" â€“ that\\'s a bit confusing. Maybe \"aep\" is a prefix or suffix? Let me think. If \"aep\" is a word, perhaps \"aep\" stands for \"apple\", so \"aeppls\" would be \"apples\". So then Acile has \"tenth apples\".\\n\\nAlternatively, maybe \"aep\" is part of a word. Let me think again. \"Aep\" â€“ maybe \"apple\". So \"aep\" is \"apple\", so \"aeppls\" is \"apples\". Therefore, Acile has \"tenth apples\". So that\\'s two words: tenth and apples.\\n\\nNow moving on to Bob\\'s case: \"futeeorn aslepp\". Again, \"futeeorn\" â€“ maybe \"fute\" with an extra \"eorn\"? Or perhaps \"fute\" with an \"eorn\" typo. Let\\'s assume \"futeeorn\" is \"fute\" with an extra \"eorn\". Maybe that\\'s \"futee\" or \"futeeorn\". If \"futee\" is a word, maybe \"futee\" is \"futee\", which could be \"futee\" â€“ but I\\'m not sure. Alternatively, \"futeeorn\" might be \"futee\" with an extra \"orn\". Let me think.\\n\\nIf \"futeeorn\" is \"futee\" with an extra \"orn\", maybe it\\'s \"futee\" â€“ but that\\'s a bit unclear. Alternatively, maybe \"futeeorn\" is \"futee\" with an \"eorn\" typo. Let\\'s take \"futee\" as the word. Then \"aslepp\" â€“ \"aslepp\" could be \"apple\" with an \"s\" and \"lepp\" â€“ maybe \"apple\" again. So Bob has \"futee\" and \"apple\", so \"futeeapple\".\\n\\nSo combining both, Acile has \"tenth apples\" and Bob has \"futeeapple\". Therefore, the total number of \"alpeps\" would be the number of \"apples\" each has. Since Acile has \"tenth apples\" which is one apple, and Bob has \"futeeapple\" which is one apple. So total is 2.\\n\\nBut wait, let me double-check. If \"tenth apples\" is \"tenth\" plus \"apples\", but \"tenth\" is a number, so maybe \"tenth apples\" is \"tenth\" as in \"ten apples\"? But the way it\\'s written is \"ttenwy aeppls\" â€“ which I thought was \"tenth aeppls\" as \"apples\". Similarly, \"futeeorn aslepp\" is \"futee\" as in \"futee\" plus \"apple\", which would be \"futeeapple\". Therefore, each has one \"apple\".\\n\\nSo total alpeps would be 1 (Acile) + 1 (Bob) = 2.\\n\\nBut let me check again. Maybe there\\'s a different interpretation. For example, \"ttenwy\" could be \"ten\" with an \"w\", making \"tenth\". \"aeppls\" could be \"apples\". So \"tenth apples\" â€“ yes, that\\'s one apple. Similarly, \"futeeorn\" â€“ maybe \"futee\" plus \"apple\" = \"futeeapple\" â€“ one apple. So total 2.\\n\\nAlternatively, if \"futeeorn\" is \"futee\" with an \"eorn\", maybe \"futee\" is \"futee\" and \"aslepp\" is \"apple\", so \"futeeapple\". So again, one apple. So total two.\\n\\nTherefore, the answer should be 2.\\n</think>\\n\\nAcile has \"tenth aeppls\" and Bob has \"futeeorn aslepp\". If \"aeppls\" is interpreted as \"apples\" and \"futeeorn\" as \"futeeapple\", then each has **one apple**. Therefore, the total number of alpeps is:\\n\\n$$\\n\\\\boxed{2}\\n$$'}]\n"
     ]
    }
   ],
   "source": [
    "qwen.reset()\n",
    "print(\n",
    "    qwen(\n",
    "        scramble_word(\n",
    "            \"Alice has twenty apples and Bob has fourteen apples. How many apples do they have in total?\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "print(qwen.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2e121fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holle! Ich kann nicht mit Ihnen verwechseln, aber ich kann Ihnen helfen. Was ist Ihnen wichtig? ðŸ˜Š\n",
      "[{'role': 'user', 'content': 'Holle. Do you uredtnnasd me?'}, {'role': 'assistant', 'content': '<think>\\nOkay, the user is asking, \"Holle. Do you uredtnnasd me?\" Let me break this down. First, \"Holle\" is German for \"Hello,\" so the user is greeting me. Then, \"Do you uredtnnasd me?\" translates to \"Do you remember me?\" in English. The user is asking if I remember them. \\n\\nNow, I need to respond appropriately. Since I\\'m an AI, I don\\'t have personal experiences or memories. So the answer should confirm that I don\\'t remember individuals. But maybe I can offer to help with something else. Let me check if there\\'s any nuance I\\'m missing. The user might be testing if I can recognize them, but I can\\'t. So the response should be straightforward but friendly.\\n\\nI should make sure the language is correct, using proper German terms. Also, keep the tone friendly and helpful. Let me put that together.\\n</think>\\n\\nHolle! Ich kann nicht mit Ihnen verwechseln, aber ich kann Ihnen helfen. Was ist Ihnen wichtig? ðŸ˜Š'}]\n"
     ]
    }
   ],
   "source": [
    "qwen.reset()\n",
    "print(qwen(scramble_word(\"Hello. Do you understand me?\")))\n",
    "print(qwen.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bcb7785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To sort the list `[54, 234, 3245, 2, 7, 3, 2]`, we need to arrange the elements in ascending order. This means comparing the numerical values and placing the smallest numbers first.\n",
      "\n",
      "After converting all elements to their numeric form, we compare them:\n",
      "\n",
      "- The smallest number is `2`.\n",
      "- The next smallest is `2`.\n",
      "- Then comes `3`, `3`.\n",
      "- Next is `54`, followed by `7`, then `234`, and finally `3245`.\n",
      "\n",
      "Thus, the sorted list is:\n",
      "\n",
      "$$\n",
      "[2, 2, 3, 3, 54, 7, 234, 3245]\n",
      "$$\n",
      "\n",
      "**Final Answer:**  \n",
      "$$\n",
      "\\boxed{[2, 2, 3, 3, 54, 7, 234, 3245]}\n",
      "$$\n",
      "[{'role': 'user', 'content': 'Sort the following list: [54, 234, 3245, 2, 7, 3, 2]'}, {'role': 'assistant', 'content': '<think>\\nOkay, let\\'s see. I need to sort the list [54, 234, 3245, 2, 7, 3, 2]. Hmm, first, I should probably check if there are any special cases or if there\\'s any specific sorting method required. The problem just says to sort it, so I guess it\\'s a regular sort.\\n\\nLet me start by writing down the list again to make sure I have it right: 54, 234, 3245, 2, 7, 3, 2. Wait, there are duplicates here. Let me check each number. 54, then 234, then 3245. Oh, 3245 is a 4-digit number. The rest are 2, 7, 3, and 2 again. So the numbers are 54, 234, 3245, 2, 7, 3, 2.\\n\\nNow, how do I sort them? Well, typically, sorting in ascending order means putting the smallest first. Let\\'s think about each number\\'s value. Let\\'s convert them to numbers to compare? Maybe that\\'s easier. Let me try converting each to a string and then sort them lexicographically? Wait, but numbers can be compared as integers. But maybe comparing them as strings would be easier for lexicographical order?\\n\\nLet me try converting each number to a string. So 54 becomes \"54\", 234 becomes \"234\", 3245 becomes \"3245\", 2 becomes \"2\", 7 becomes \"7\", 3 becomes \"3\", and 2 again is \"2\". Now, if I sort these strings, they would be in order: \"2\", \"2\", \"3\", \"3\", \"54\", \"234\", \"3245\". Wait, but that\\'s not sorted in numerical order. Because \"2\" is less than \"3\", which is less than \"3\"? No, \"3\" is the same as \"3\". So the order would be \"2\", \"2\", \"3\", \"3\", \"54\", \"234\", \"3245\". But that\\'s not correct because \"54\" is 54, \"234\" is 234, which is larger than \"54\". So the sorted order would be \"2\", \"2\", \"3\", \"3\", \"54\", \"234\", \"3245\". But wait, that\\'s not sorted numerically. Because 234 is larger than 54, so the correct order would be sorted numerically. So maybe I should sort them as numbers first, then check duplicates. But if duplicates are allowed, maybe the order is the same as the original? But the problem says to sort, so maybe I need to sort them numerically.\\n\\nWait, let me try sorting them as numbers. Let\\'s list them again:\\n\\nOriginal list: [54, 234, 3245, 2, 7, 3, 2]\\n\\nIf I sort them numerically, the order would be 2, 2, 3, 3, 54, 7, 234, 3245. Wait, but 7 is after 3. So the correct sorted order would be [2, 2, 3, 3, 54, 7, 234, 3245]. But wait, that\\'s not correct either. Because 54 is 54, which is less than 7? No, 54 is 54, which is greater than 7. So 54 comes after 7. So the correct order would be [2, 2, 3, 3, 54, 7, 234, 3245]. Wait, but 54 is 54, which is greater than 7. So the correct sorted order is 2, 2, 3, 3, 54, 7, 234, 3245.\\n\\nBut wait, 7 is after 54. So the order would be 2, 2, 3, 3, 54, 7, 234, 3245. But is there a mistake here? Let me check again. The original numbers are 54, 234, 3245, 2, 7, 3, 2. So sorted numerically, they should be:\\n\\n2, 2, 3, 3, 54, 7, 234, 3245. Yes, that\\'s correct. Because 2 is the smallest, then 2 again, then 3, then 3, then 54, then 7, then 234, then 3245.\\n\\nBut wait, 7 is 7, which is less than 54, so after 54 comes 7. So the correct sorted order is as above.\\n\\nAlternatively, if I had to sort them as strings, the order would be \"2\", \"2\", \"3\", \"3\", \"54\", \"7\", \"234\", \"3245\". Which is the same as the sorted numbers, but in a different order. But since the problem says to sort, and doesn\\'t specify the order, perhaps the numerical order is expected.\\n\\nBut maybe there\\'s another way. Let me check each number again. Let\\'s list them as numbers:\\n\\nOriginal numbers: 54, 234, 3245, 2, 7, 3, 2.\\n\\nIf I sort them, they should be:\\n\\n2, 2, 3, 3, 54, 7, 234, 3245.\\n\\nBut wait, 54 is 54, which is greater than 7. So yes, 54 comes after 7. So the correct order is as above.\\n\\nAlternatively, maybe I should check if there\\'s any other way to sort. For example, if the numbers are in a specific order, but since the problem doesn\\'t specify, I think the standard approach is to sort numerically. Therefore, the sorted list would be [2, 2, 3, 3, 54, 7, 234, 3245].\\n\\nBut let me verify once more. Let\\'s write them all out:\\n\\nOriginal list: 54, 234, 3245, 2, 7, 3, 2.\\n\\nAfter sorting: 2, 2, 3, 3, 54, 7, 234, 3245.\\n\\nYes, that\\'s correct. Therefore, the sorted list is [2, 2, 3, 3, 54, 7, 234, 3245].\\n</think>\\n\\nTo sort the list `[54, 234, 3245, 2, 7, 3, 2]`, we need to arrange the elements in ascending order. This means comparing the numerical values and placing the smallest numbers first.\\n\\nAfter converting all elements to their numeric form, we compare them:\\n\\n- The smallest number is `2`.\\n- The next smallest is `2`.\\n- Then comes `3`, `3`.\\n- Next is `54`, followed by `7`, then `234`, and finally `3245`.\\n\\nThus, the sorted list is:\\n\\n$$\\n[2, 2, 3, 3, 54, 7, 234, 3245]\\n$$\\n\\n**Final Answer:**  \\n$$\\n\\\\boxed{[2, 2, 3, 3, 54, 7, 234, 3245]}\\n$$'}]\n"
     ]
    }
   ],
   "source": [
    "qwen.reset()\n",
    "print(qwen(\"Sort the following list: [54, 234, 3245, 2, 7, 3, 2]\"))\n",
    "print(qwen.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "157be993",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m qwen.reset()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mqwen\u001b[49m\u001b[43m(\u001b[49m\u001b[43madd_characters\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDo you understand me?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mQwenChatbot.__call__\u001b[39m\u001b[34m(self, user_input, enable_thinking)\u001b[39m\n\u001b[32m     19\u001b[39m text = \u001b[38;5;28mself\u001b[39m.tokenizer.apply_chat_template(\n\u001b[32m     20\u001b[39m     messages,\n\u001b[32m     21\u001b[39m     tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     22\u001b[39m     add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     23\u001b[39m     enable_thinking=enable_thinking,\n\u001b[32m     24\u001b[39m )\n\u001b[32m     26\u001b[39m inputs = \u001b[38;5;28mself\u001b[39m.tokenizer(text, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m response_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32768\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m][\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mlen\u001b[39m(inputs.input_ids[\u001b[32m0\u001b[39m]) :\n\u001b[32m     29\u001b[39m ].tolist()\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# rindex finding 151668 (</think>)\u001b[39;00m\n\u001b[32m     33\u001b[39m     split_index = \u001b[38;5;28mlen\u001b[39m(response_ids) - response_ids[::-\u001b[32m1\u001b[39m].index(\u001b[32m151668\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2629\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2621\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2622\u001b[39m         input_ids=input_ids,\n\u001b[32m   2623\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2624\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2625\u001b[39m         **model_kwargs,\n\u001b[32m   2626\u001b[39m     )\n\u001b[32m   2628\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2630\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2634\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2636\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2639\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2640\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2641\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2642\u001b[39m         input_ids=input_ids,\n\u001b[32m   2643\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2644\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2645\u001b[39m         **model_kwargs,\n\u001b[32m   2646\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:3613\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3611\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3612\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3613\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3615\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3616\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3617\u001b[39m     outputs,\n\u001b[32m   3618\u001b[39m     model_kwargs,\n\u001b[32m   3619\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3620\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:959\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    957\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    958\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m959\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    961\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:481\u001b[39m, in \u001b[36mQwen3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    444\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    458\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    459\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m    461\u001b[39m \u001b[33;03m        Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    479\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    492\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    493\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:1083\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1080\u001b[39m                 module.forward = make_capture_wrapper(module, original_forward, key, specs.index)\n\u001b[32m   1081\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[32m   1085\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:405\u001b[39m, in \u001b[36mQwen3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    402\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    418\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    419\u001b[39m     past_key_values=past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    420\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:272\u001b[39m, in \u001b[36mQwen3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m residual = hidden_states\n\u001b[32m    271\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:81\u001b[39m, in \u001b[36mQwen3MLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     down_proj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/small-llm-experiments/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "qwen.reset()\n",
    "print(qwen(add_characters(\"Do you understand me?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ccf50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What jxvmXY is seSSEy the NLVWyP square tuxbop root lWslJI of uRCnqy two? yYCCyg'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_words(\"What is the square root of two?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999234ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question seems to be a riddle or a cryptic puzzle. Let's break it down:\n",
      "\n",
      "- **\"What OQdao is ICPWbZ the PISiaK square HioQcF root jqFCPr of vYCzoH two?\"**\n",
      "\n",
      "### Step-by-step:\n",
      "\n",
      "1. **\"OQdao\"** â€“ This could be a cipher or a reference to a word.\n",
      "2. **\"ICPWbZ\"** â€“ This might be a number or a code.\n",
      "3. **\"PISiaK\"** â€“ This could be a word or a cipher.\n",
      "4. **\"HioQcF\"** â€“ Another cipher or code.\n",
      "5. **\"jqFCPr\"** â€“ Another cipher or code.\n",
      "6. **\"vYCzoH\"** â€“ Another cipher or code.\n",
      "\n",
      "### Possible Interpretation:\n",
      "\n",
      "This riddle is likely a play on words or a cryptic puzzle where the answer is a word or phrase that can be derived from the cipher codes.\n",
      "\n",
      "### Final Answer:\n",
      "\n",
      "**\"Two\"** â€“ This fits the pattern, as it's the only word that can be derived from the cipher codes.\n",
      "\n",
      "**Answer:** Two.\n"
     ]
    }
   ],
   "source": [
    "qwen.reset()\n",
    "print(qwen(add_words(\"What is the square root of two?\"), enable_thinking=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c3b217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Do nIYviD you aYSjuR understand yDtqIL me? PNxbgd'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"<think>\\nOkay, the user is asking if I understand them. First, I need to respond politely. Since I'm an AI, I should acknowledge their question and express that I'm here to help. I should keep it friendly and open-ended so they feel comfortable sharing more. Let me make sure the response is clear and that they know I'm available for any questions they might have. I should also check if there's anything else they need, so they know I'm here to assist further.\\n</think>\\n\\nHello! I'm here to help you with anything you need. How can I assist you today? ðŸ˜Š\"}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693ab8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dxox xyxoxux xuxnxdxexrxsxtxaxnxdx xmxex?x'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_characters(\"Do you understand me?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b71284f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151644 <|im_start|>\n",
      "872 user\n",
      "198 \n",
      "\n",
      "5404 Do\n",
      "498  you\n",
      "3535  understand\n",
      "752  me\n",
      "30 ?\n",
      "151645 <|im_end|>\n",
      "198 \n",
      "\n",
      "151644 <|im_start|>\n",
      "77091 assistant\n",
      "198 \n",
      "\n",
      "151667 <think>\n",
      "271 \n",
      "\n",
      "\n",
      "151668 </think>\n",
      "271 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token = qwen.tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"Do you understand me?\"}],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,\n",
    ")\n",
    "\n",
    "for t in token:\n",
    "    print(t, qwen.tokenizer.decode(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "504af052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151644,\n",
       " 872,\n",
       " 198,\n",
       " 5404,\n",
       " 498,\n",
       " 3535,\n",
       " 752,\n",
       " 30,\n",
       " 151645,\n",
       " 198,\n",
       " 151644,\n",
       " 77091,\n",
       " 198,\n",
       " 151667,\n",
       " 271,\n",
       " 151668,\n",
       " 271]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = qwen.tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"Do you understand me?\"}],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,\n",
    ")\n",
    "list(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "255e5f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151644,\n",
       " 872,\n",
       " 198,\n",
       " 35,\n",
       " 87,\n",
       " 5131,\n",
       " 30784,\n",
       " 87,\n",
       " 5131,\n",
       " 2200,\n",
       " 856,\n",
       " 2200,\n",
       " 77,\n",
       " 9703,\n",
       " 87,\n",
       " 327,\n",
       " 12651,\n",
       " 82,\n",
       " 2252,\n",
       " 87,\n",
       " 706,\n",
       " 77,\n",
       " 9703,\n",
       " 87,\n",
       " 856,\n",
       " 18085,\n",
       " 327,\n",
       " 30,\n",
       " 87,\n",
       " 151645,\n",
       " 198,\n",
       " 151644,\n",
       " 77091,\n",
       " 198,\n",
       " 151667,\n",
       " 271,\n",
       " 151668,\n",
       " 271]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = qwen.tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": add_characters(\"Do you understand me?\")}],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False,\n",
    ")\n",
    "list(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d3f2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
